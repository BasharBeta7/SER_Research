{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook is to prepare audio speech from the dataset \"IEMOCAP\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> first, we will prepare folder tree, we implement this by copying necessary audio files (sentences) from the dataset into a specified directory (TARGET_PATH). folder tree is as follows:\n",
    "    \n",
    "    root:\n",
    "    ---- /IEMOCAP\n",
    "    ---- ---- /Session1\n",
    "    ---- ---- ---- /EmoEval\n",
    "    ---- ---- ---- /Sentences\n",
    "    ---- ---- /Session2\n",
    "    ---- ---- ---- /EmoEval\n",
    "    ---- ---- ---- /Sentences\n",
    "    ---- ---- /Session3\n",
    "    ---- ---- ---- /EmoEval\n",
    "    ---- ---- ---- /Sentences\n",
    "    ---- ---- /Session4\n",
    "    ---- ---- ---- /EmoEval\n",
    "    ---- ---- ---- /Sentences\n",
    "    ---- ---- /Session5\n",
    "    ---- ---- ---- /EmoEval\n",
    "    ---- ---- ---- /Sentences\n",
    "    </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to import all necessary packages\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import shutil\n",
    "\n",
    "#constant files are global UPPERCASE\n",
    "TARGET_PATH = \"../Datasets/IEMOCAP\"\n",
    "SRC_PATH = \"../Datasets/IEMOCAP_full_release\"\n",
    "\n",
    "def prepare_directory(src_dataset, trg_dataset):\n",
    "    \"\"\"\n",
    "    prepare dataset_folder as folllow:\n",
    "    trg_dataset contains 5 sessions, each session folder containes 2 subfolders : EmoEval, Sentences\n",
    "    - EmoEval has the values of (filename, emotion label)\n",
    "    - Sentences containes the .wav files \n",
    "    \"\"\"\n",
    "    os.makedirs(trg_dataset, exist_ok=True)\n",
    "    for dir_path, dir_names, filenames in os.walk(src_dataset):\n",
    "        folder_names = dir_path.split(sep=\"/\")\n",
    "        \n",
    "        if folder_names[-1] == \"EmoEvaluation\":\n",
    "            for file in filenames:\n",
    "                target_path = os.path.join(trg_dataset,folder_names[-3],\"EmoEval\")\n",
    "                os.makedirs(target_path, exist_ok=True)\n",
    "                current_path = os.path.join(dir_path, file)\n",
    "                shutil.copy(current_path, target_path)\n",
    "                \n",
    "                \n",
    "        if folder_names[-2] == \"sentences\" and folder_names[-1] == 'wav':\n",
    "            target_path = os.path.join(trg_dataset,folder_names[-3],\"Sentences\")\n",
    "            for d in dir_names:\n",
    "                current_path = os.path.join(dir_path, d)\n",
    "                shutil.copytree(current_path, target_path, symlinks=False, ignore=None, ignore_dangling_symlinks=False, dirs_exist_ok=True)\n",
    "\n",
    "    \n",
    "prepare_directory(SRC_PATH, TARGET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 9 target classification were recorded in the dataset, first we will enumerate them to get a dictionary {emotion : label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"angry, happy, sad, neutral, frustrated, excited, fearful, disgusted, other\".split(\",\")\n",
    "y = sorted(y)\n",
    "y = [i.strip() for i in y]\n",
    "x = [f[0:3] for f in y]\n",
    "x = dict.fromkeys(x)\n",
    "for i, j in enumerate(x.keys()):\n",
    "    x[j] = i\n",
    "    \n",
    "label_index = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Folder: ../Datasets/IEMOCAP/Session3/EmoEval: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 2147.48it/s]\n",
      "Reading Folder: ../Datasets/IEMOCAP/Session5/EmoEval: 100%|███████████████████████████████████████████████| 31/31 [00:00<00:00, 2685.77it/s]\n",
      "Reading Folder: ../Datasets/IEMOCAP/Session4/EmoEval: 100%|███████████████████████████████████████████████| 30/30 [00:00<00:00, 3025.90it/s]\n",
      "Reading Folder: ../Datasets/IEMOCAP/Session1/EmoEval: 100%|███████████████████████████████████████████████| 28/28 [00:00<00:00, 2398.75it/s]\n",
      "Reading Folder: ../Datasets/IEMOCAP/Session2/EmoEval: 100%|███████████████████████████████████████████████| 30/30 [00:00<00:00, 3139.61it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Datasets/IEMOCAP/json/data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput location: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trg_path))\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filenames_labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJSON_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36mprepare_json\u001b[0;34m(src_path, trg_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m         current_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialog/EmoEvaluation/Categorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(track_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m     53\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(filenames_labels, fp, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput location: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trg_path))\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Datasets/IEMOCAP/json/data.json'"
     ]
    }
   ],
   "source": [
    "#prepare json file that stores filenames, genders, and labels from all sessions of the dataset\n",
    "\n",
    "\n",
    "ROOT_PATH = \"../Datasets/IEMOCAP\"\n",
    "JSON_PATH = \"../Datasets/IEMOCAP/json/data.json\"\n",
    "\n",
    "\n",
    "def prepare_json(src_path, trg_path):\n",
    "        \n",
    "    \n",
    "    global filenames_labels\n",
    "    filenames_labels = {\n",
    "        \"filenames\" : [],\n",
    "        \"genders\" : [],\n",
    "        \"labels\" : []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for dir_path, dir_names, filenames in os.walk(src_path):\n",
    "        # if current dir is Session_i      \n",
    "        folder_names = dir_path.split(sep=\"/\")\n",
    "        if folder_names[-1] == \"EmoEval\":\n",
    "            for f in tqdm.tqdm(filenames, desc=\"Reading Folder: {}\".format(dir_path), ncols=140):\n",
    "                if f.split(\".\")[-1] == \"txt\":\n",
    "                    current_path = os.path.join(dir_path, f)\n",
    "                    with open(current_path, \"r\") as fr:\n",
    "                        for l in fr:\n",
    "                            reg = re.findall(\"\\[\\d+.\\d+ \\- \\d+.\\d+\\]\", l)\n",
    "                            \n",
    "                            if reg is not None:\n",
    "                                splits = l.split()\n",
    "                                if len(splits) < 5:\n",
    "                                    continue\n",
    "                                sentence_path = splits[3]\n",
    "                                emo = splits[4]\n",
    "                                \n",
    "                                if emo in label_index:\n",
    "                                    track_path = os.path.join(dir_path, '../Sentences', sentence_path + \".wav\")\n",
    "                                    try:\n",
    "                                        librosa.load(track_path)\n",
    "                                        filenames_labels[\"filenames\"].append(track_path)\n",
    "                                        filenames_labels[\"genders\"].append(sentence_path[-4])\n",
    "                                        filenames_labels[\"labels\"].append(label_index[emo])\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                \n",
    "        if dir_path.split(sep=\"/\")[-1][:-1] == \"Session\" :\n",
    "            session_number = dir_path.split(sep=\"/\")[-1][-1]\n",
    "            current_path = os.path.join(dir_path, 'dialog/EmoEvaluation/Categorical')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(track_path), exist_ok=True)\n",
    "    with open(trg_path, 'w') as fp:\n",
    "        json.dump(filenames_labels, fp, indent=4)\n",
    "    \n",
    "    \n",
    "    print(\"output location: {}\".format(trg_path))\n",
    "    return filenames_labels[\"labels\"]\n",
    "        \n",
    "        \n",
    "files = prepare_json(ROOT_PATH, JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7135"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction:\n",
    "\n",
    "for each speech utternace U = {u_1, u_2 ... u_N} : u_i is a vector of 26 features (13 MFCC + 13 Delta coefficients). \n",
    "N is the total number of frames for each utterance input (fixed, default is 50, each frame contains 100ms of audio ==> each sample input represents 5 seconds of speech).\n",
    "\n",
    "input : Raw audio file (utterance) with shape (1, T) where T is the length in seconds.\n",
    "output : set of shape (N, d_input)  N is Sequence Length, d_input is feature vector \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "SAMPLE_RATE=22050\n",
    "\n",
    "\n",
    "def extract_features(waveform, sr, win_size=0.025, stride=0.01, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    initially we will use 13 MFCC + 13 Delta as low level descriptors \n",
    "    :param track_path: path of the audio track\n",
    "    :param win_size: length of the hamming window (in seconds)\n",
    "    :param stride: stride length (in seconds)\n",
    "    :param n_mfcc: number of desired mel frequency cepstral coefficients\n",
    "    :returns : spectogram of mfcc+elta features\n",
    "    \"\"\"\n",
    "    \n",
    "    #specifty parameters of mfcc\n",
    "    n_fft = int(win_size*sr)\n",
    "    hop_length = int(stride*sr)\n",
    "    \n",
    "    # get mfcc, deltas \n",
    "    MFCC = librosa.feature.mfcc(y=waveform,\n",
    "                                n_mfcc=n_mfcc,\n",
    "                                n_fft=n_fft,\n",
    "                                hop_length=hop_length)\n",
    "    # get the\n",
    "    MFCC_delta = librosa.feature.delta(MFCC)\n",
    "    #librosa.display.specshow(MFCC, x_axis=\"time\")\n",
    "    #plt.colorbar()\n",
    "    #plt.xlabel('time')\n",
    "    #plt.ylabel('MFCC')\n",
    "    \n",
    "    MFCC = MFCC\n",
    "    MFCC_delta = MFCC_delta\n",
    "    \n",
    "    features = np.vstack((MFCC, MFCC_delta))\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "audio data preprocessing consists of : voice activity detection to remove silences from beginning and end of each audio utterance. then data-segmenting to obtain fixed-length audio utterances as inputs. Each utterance is windowed into many segments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will adapt similar approach to : \n",
    "@ARTICLE{gong_psla, \n",
    "    author={Gong, Yuan and Chung, Yu-An and Glass, James},  \n",
    "    journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   \n",
    "    title={PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation},   \n",
    "    year={2021}, \n",
    "    doi={10.1109/TASLP.2021.3120633}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting features from audio : 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished jsonifying, output directory is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trg_path_json))\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m---> 49\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_to_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilenames_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_path_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m, in \u001b[0;36mfeature_to_json\u001b[0;34m(filenames, trg_path_json, n_mfcc)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         failed_names\u001b[38;5;241m.\u001b[39mappend(track)\n\u001b[0;32m---> 37\u001b[0m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m audio tracks have been processed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(successful_iterations, total))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# extract features into output dir\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2791\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2677\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "#extract features into json file \n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "def feature_to_json(filenames: dict, trg_path_json, n_mfcc=13 ):\n",
    "    \"\"\"\n",
    "    accepts dictonary filenames_labels and extract features of all including tracks to a json outputfile\"\"\"\n",
    "    \n",
    "    output = {\n",
    "        \"dataset\" : \"IEMOCAP\",\n",
    "        \"n_mfcc\": int(n_mfcc),\n",
    "        \"features\" : [\"mfcc\", \"mfcc-delta\"],     \n",
    "        \"MFCCs\" : [],\n",
    "        \"label\" : [],\n",
    "        \"n_features\":0,\n",
    "        \"n_classes\" : 0,\n",
    "    }\n",
    "    \n",
    "    # read filenames\n",
    "    failed_names = []\n",
    "    successful_iterations = 0\n",
    "    total = len(filenames[\"labels\"])\n",
    "    \n",
    "    for it, track in tqdm.tqdm(enumerate(filenames[\"filenames\"]), desc=\"extracting features from audio \",ncols=140, total=total):\n",
    "        try:\n",
    "    \n",
    "            features = extract_features(track, n_mfcc=n_mfcc)\n",
    "            waveform = features[1]\n",
    "            features = features[0]\n",
    "            output[\"MFCCs\"].append(features.tolist())\n",
    "            output[\"label\"].append(filenames[\"labels\"][it])\n",
    "            output[\"n_features\"] = int(features.shape[0])\n",
    "            successful_iterations += 1\n",
    "        except:\n",
    "            failed_names.append(track)\n",
    "    \n",
    "    output[\"n_classes\"] = int(np.max(output[\"label\"], 0) + 1)\n",
    "    print(\"{}/{} audio tracks have been processed successfully\".format(successful_iterations, total))\n",
    "        \n",
    "    # extract features into output dir\n",
    "    print(\"dumping to json file...\")\n",
    "    with open(trg_path_json, 'w') as fp:\n",
    "        json.dump(output, fp, indent=4)\n",
    "    \n",
    "    print(\"Finished jsonifying, output directory is {}\".format(trg_path_json))\n",
    "    return output\n",
    "    \n",
    "    \n",
    "dataset = feature_to_json(filenames=filenames_labels, trg_path_json=\"./results.json\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaveforms\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "len(dataset[\"waveforms\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SER architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from json file...\n",
      "data loaded successfully\n",
      "dataset derived from : IEMOCAP\n",
      "total number of samples in set: 7135\n",
      "features used: ['mfcc', 'mfcc-delta']\n",
      "number of target classes: 9\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class SER_Dataset(Dataset):\n",
    "    \"\"\"this class loads waveform and labels from a json file, and process it to extract necessary features\"\"\"\n",
    "    def __init__(self, json_config_file):\n",
    "        \"\"\"\n",
    "        :param json_config_file is of shape {waveforms: [], labels: []}\"\"\"\n",
    "        \n",
    "        # load json data to local\n",
    "        print(\"loading data from json file...\")\n",
    "        with open(json_config_file, 'r') as fp:\n",
    "            data = json.load(fp)\n",
    "        print(\"data loaded successfully\")\n",
    "        \n",
    "        self.dataset = data[\"dataset\"]\n",
    "        self.n_features = data[\"n_features\"]\n",
    "        self.n_samples = len(data[\"label\"])\n",
    "        self.features_extracted = data[\"features\"]\n",
    "        self.n_classes = data[\"n_classes\"]\n",
    "        self.data = data[\"MFCCs\"]\n",
    "        self.targets = data[\"label\"]\n",
    "        \n",
    "        # print information about the data : number of samples, features, number of classes\n",
    "        print(\"dataset derived from : {}\".format(self.dataset))\n",
    "        print(\"total number of samples in set: {}\".format(self.n_samples))\n",
    "        print(\"features used: {}\".format(self.features_extracted))\n",
    "        print(\"number of target classes: {}\".format(self.n_classes))\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return np.array(self.data[index]), self.targets[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "\n",
    "my_set = SER_Dataset(\"./results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## create dataset with fixed_length signals for easier processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_MFCC_delta(waveform, sr, win_size=0.025, stride=0.01, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    initially we will use 13 MFCC + 13 Delta as low level descriptors \n",
    "    :param track_path: path of the audio track\n",
    "    :param win_size: length of the hamming window (in seconds)\n",
    "    :param stride: stride length (in seconds)\n",
    "    :param n_mfcc: number of desired mel frequency cepstral coefficients\n",
    "    :returns : spectogram of mfcc+elta features\n",
    "    \"\"\"\n",
    "    \n",
    "    #specifty parameters of mfcc\n",
    "    n_fft = int(win_size*sr)\n",
    "    hop_length = int(stride*sr)\n",
    "    \n",
    "    # get mfcc, deltas \n",
    "    MFCC = librosa.feature.mfcc(y=waveform,\n",
    "                                n_mfcc=n_mfcc,\n",
    "                                n_fft=n_fft,\n",
    "                                hop_length=hop_length)\n",
    "    \n",
    "    MFCC_delta = librosa.feature.delta(MFCC)\n",
    "    #librosa.display.specshow(MFCC, x_axis=\"time\")\n",
    "    #plt.colorbar()\n",
    "    #plt.xlabel('time')\n",
    "    #plt.ylabel('MFCC')\n",
    "    \n",
    "    MFCC = MFCC\n",
    "    MFCC_delta = MFCC_delta\n",
    "    \n",
    "    features = np.vstack((MFCC, MFCC_delta))\n",
    "    print(\"features shape is : {}\".format(features.shape))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.52478552e-04, 1.48494318e-02, 1.58146140e-04, ...,\n",
       "        2.39356887e-03, 9.67159495e-03, 3.36409220e-03],\n",
       "       [6.00285223e-03, 1.34558845e-02, 1.49821187e-03, ...,\n",
       "        1.80263328e-03, 8.19785800e-03, 3.04659340e-03],\n",
       "       [1.99372172e-02, 8.61566141e-03, 5.12828398e-03, ...,\n",
       "        2.70832079e-05, 3.50711984e-03, 1.94517721e-03],\n",
       "       ...,\n",
       "       [1.88185368e-07, 3.91738382e-07, 3.24324077e-07, ...,\n",
       "        1.64962717e-06, 2.87581133e-06, 3.83244605e-06],\n",
       "       [1.24415592e-07, 6.52505108e-08, 7.93541659e-08, ...,\n",
       "        2.99635474e-07, 1.58410913e-07, 4.11869223e-06],\n",
       "       [2.33628612e-07, 9.12033826e-08, 6.63071020e-08, ...,\n",
       "        1.33230188e-07, 5.96501977e-08, 3.51766357e-06]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def extract_mel_bins(waveform, sr, win_size=0.025, stride=0.01, n_mels= 64):\n",
    "    mel_spectorgram = torchaudio.transforms.MelSpectrogram(sample_rate=sr,\n",
    "                                                    n_fft=int(win_size*sr),\n",
    "                                                   hop_length=int(stride*sr),\n",
    "                                                   n_mels= n_mels)\n",
    "    features = mel_spectorgram(waveform)\n",
    "    features = np.array(features)\n",
    "    features = np.squeeze(features, axis=0)\n",
    "    return features\n",
    "\n",
    "waveform, sr = torchaudio.load(\"/home/bashar/Study/Research_SER/Notebook/Ses03F_impro01_F004.wav\")\n",
    "extract_mel_bins(waveform, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 364)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "signal, sr = torchaudio.load(\"/home/bashar/Study/Research_SER/Notebook/Ses03F_impro01_F004.wav\")\n",
    "\n",
    "padded = torch.nn.functional.pad(signal, (0, 20))\n",
    "\n",
    "\n",
    "extract_mel_bins(padded, sr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "SAMPLE_RATE=22050\n",
    "NUM_SAMPLES = 3 * SAMPLE_RATE\n",
    "JSON_PATH = \"../Datasets/IEMOCAP/json/data.json\"\n",
    "\n",
    "class SER_dataset(Dataset):\n",
    "    def pad_cut_if_necessary(self, waveform):\n",
    "        # check length of wave form\n",
    "        wav_length = waveform.shape[1]\n",
    "        \n",
    "        # if longer, cut the waveform\n",
    "        if wav_length > self.num_samples:\n",
    "            waveform = waveform[:, :self.num_samples]\n",
    "            \n",
    "        # if shorter, pad the waveform with zeros\n",
    "        if wav_length < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - wav_length\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, num_missing_samples))\n",
    "   \n",
    "        return waveform\n",
    "  \n",
    "    \n",
    "    def __init__(self, json_config_file, transformation,  num_samples=NUM_SAMPLES):\n",
    "        \"\"\"\n",
    "        :param json_config_file : .json file which includes tracks paths and their labels\n",
    "        :param transformation : a function pointer that will be used to extract features from input audio signals\n",
    "        :num_samples : total number of samples allowed for the input\"\"\"\n",
    "        with open(json_config_file, 'r') as pf:\n",
    "            data = json.load(pf)\n",
    "            \n",
    "        self.filenames = data[\"filenames\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "        self.num_samples = num_samples\n",
    "        self.transformation = transformation\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # get waveform from current index\n",
    "        waveform, sr = torchaudio.load(self.filenames[index])\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq= SAMPLE_RATE)\n",
    "        waveform = resampler(waveform)\n",
    "        waveform = (waveform)\n",
    "        waveform = self.pad_cut_if_necessary(waveform)\n",
    "        features = self.transformation(waveform, sr)\n",
    "        \n",
    "        # get label of the current index\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return features, label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7.28028186e-04, 7.92935584e-03, 2.41690315e-03, ...,\n",
       "         2.82597292e-04, 2.40223925e-03, 1.59090552e-02],\n",
       "        [5.83066139e-04, 6.81230566e-03, 2.07271893e-03, ...,\n",
       "         2.29460318e-04, 2.28847400e-03, 1.22437123e-02],\n",
       "        [1.37512412e-04, 3.21439281e-03, 9.65998799e-04, ...,\n",
       "         6.50228467e-05, 1.80890015e-03, 1.15526537e-03],\n",
       "        ...,\n",
       "        [9.72811591e-08, 7.84727128e-09, 1.08825287e-08, ...,\n",
       "         7.89695953e-09, 1.38550735e-08, 2.69502402e-06],\n",
       "        [5.97432859e-08, 2.10878670e-09, 1.47968982e-09, ...,\n",
       "         1.16064758e-09, 1.62070479e-09, 2.61602918e-06],\n",
       "        [7.60158017e-08, 4.96486807e-09, 4.27872893e-09, ...,\n",
       "         8.42900472e-10, 1.47550061e-09, 2.64305868e-06]], dtype=float32),\n",
       " 5)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_set = SER_dataset(JSON_PATH, transformation=extract_mel_bins)\n",
    "my_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model \n",
    "\n",
    "this layer applies 1d convolution on the spectogram, to get the embeddings of each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 128, 1, 40]         131,200\n",
      "================================================================\n",
      "Total params: 131,200\n",
      "Trainable params: 131,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.50\n",
      "Estimated Total Size (MB): 0.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "class input_embedding(nn.Module):\n",
    "    def __init__(self, d_input, d_model, stride=10, kernel_size=16):\n",
    "        \"\"\"\n",
    "        :param d_input: feature-dimension of input spectogram\n",
    "        :param d_model: output dimension of the embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(1, d_model, kernel_size=(d_input, kernel_size), stride=stride)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: input of shape [B, d_input, t_length]\n",
    "        return : output of shape [B, max_length, d_model]\"\"\"\n",
    "        # we need to add new axis for number of channels\n",
    "        x = torch.unsqueeze(x, dim=1)\n",
    "        # we apply the projection layer\n",
    "        x = self.proj(x)\n",
    "        # we remove the height dimension, because it is always equal to 1 \n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        #x = torch.einsum('bij->bji', x)\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "    \n",
    "inp = torch.randn([5, 64, 414])\n",
    "embed = input_embedding(64, 128)\n",
    "summary(embed, (64, 414))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will work on positional encoding PE, which is added to the output of the projection layer (AKA embeddings) to capture the positional information of the input audio.\n",
    "the theory behind this positional encoding techniques is derived from the paper \"Attention is all you need\"\n",
    "\n",
    "PE(pos,2i) = sin(pos/(1000^(2i/d_model))\n",
    "PE(pos,2i+1) = cos(pos/(1000^(2i/d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding :\n",
    "from torchsummary import summary\n",
    "\n",
    "class PositionalEncoding( nn.Module ):\n",
    "    \"\"\"\n",
    "    a nn.Module wrapper to extract Position embeddings with a specific dimnesionality\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model, max_length):\n",
    "        \"\"\":param max_length: number of columns of input embedding [not used for now]\n",
    "        :param d_model: number of rows to represent input features of the transformer\"\"\"\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # get the length of sequency from input\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        even_den = pow(10000, even_i/self.d_model)\n",
    "        pos = torch.arange(seq_len).float().unsqueeze(1)\n",
    "        \n",
    "        even_pe = torch.sin(pos/even_den)\n",
    "        odd_pe = torch.cos(pos/even_den)\n",
    "        stacked = torch.stack([even_pe, odd_pe], dim=2)\n",
    "        stacked = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return stacked\n",
    " \n",
    "pe = PositionalEncoding(128,40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build the Transformer encoder blocks, which consists of multi-headed attention layer, followed by a feed-forward layer (as in the paper \"Attention is all you need\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadSplit(nn.Module):\n",
    "    \"\"\"split the input data into multiple heads using linear layers\"\"\"\n",
    "    def __init__(self, d_model:int, d_k:int, heads:int= 8, bias=False):\n",
    "        \"\"\"\n",
    "        :param d_model: input feature dimension\n",
    "        :param d_k: dimension of each head \n",
    "        :param heads: number of heads to split each input sample into\n",
    "        :param bias: whether to apply bias term into linear layer\"\"\"\n",
    "        super().__init__()\n",
    "        # copy params into local attributes of the class\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.heads = heads\n",
    "\n",
    "        # assert that numbers check\n",
    "        assert(self.d_k * self.heads == self.d_model), \"heads number isn't compatible with dimesions!\"\n",
    "        \n",
    "        \n",
    "        # create a linear layer to create the different heads\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"x is of shape [-1, seq_length, d_model]\"\"\"\n",
    "        # change the view of the input so that trainable features (d_model) is the last\n",
    "        shape = x.shape[:-1]\n",
    "        # split x into multiple heads\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        \n",
    "        # split last dimension into two \n",
    "        x = x.view(*shape, self.heads, self.d_k)\n",
    "        x = torch.transpose(x, -2, -3)\n",
    "        \n",
    "\n",
    "        # return output\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 40, 128]          16,384\n",
      "================================================================\n",
      "Total params: 16,384\n",
      "Trainable params: 16,384\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mh = MultiHeadSplit(128, 16)\n",
    "summary(mh, (40, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we will create the scaled dot-product attention layer, which will be used to extract meaningful relations between different parts of the audio track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_k, bias=False, dropout=0.1): # dropout is set randomly \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # split input into three matrecies : q: query, k: key, v: value using linear layers\n",
    "        self.query = MultiHeadSplit(d_model, d_k, heads, bias)\n",
    "        self.key = MultiHeadSplit(d_model, d_k, heads, bias)\n",
    "        self.value = MultiHeadSplit(d_model, d_k, heads, bias)\n",
    "        \n",
    "        # define necessery activations\n",
    "        self.softmax = nn.Softmax(dim= -1) # the input to the softmax has shape (t_dim, t_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = 1 / (self.d_k)**(0.5)\n",
    "        \n",
    "        output = nn\n",
    "        \n",
    "    def get_att(self, q, k):\n",
    "        \"\"\"\n",
    "        q shape (B, h, q_len, d)\n",
    "        k shape (B, h, k_len, d)\n",
    "        \n",
    "        :return : output of shape (B, h, q_len, k_len)\n",
    "            \"\"\"\n",
    "        out = torch.einsum('bhqd,bhkd->bhqk', q, k)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        :param x : input of shape (B, seq_length, d_model) \n",
    "        :param mask : mask of shape (B, seq_length)\n",
    "        \n",
    "        :return output of shape (B, seq_length, d_v)\n",
    "        \"\"\"\n",
    "                \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # apply matmul\n",
    "        att = self.get_att(q, k)\n",
    "        \n",
    "        # apply scale\n",
    "        att = att * self.scale\n",
    "        \n",
    "      \n",
    "        \n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            att = att.masked_fill(mask==0, -1 * torch.inf)\n",
    "            \n",
    "        # apply softmax\n",
    "        att = self.softmax(att)\n",
    "        \n",
    "        # apply matmul\n",
    "        att = torch.einsum('bhqk,bhkd->bqhd', att, v)\n",
    "        \n",
    "        # concatenate all heads in one (bhqd -> bq(d*h)\n",
    "        shape = att.shape[:-2]\n",
    "\n",
    "        out = att.reshape(*shape, att.shape[-1] * att.shape[-2])\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "  \n",
    "        \n",
    "from torchsummary import summary \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 40, 1024]         132,096\n",
      "           Dropout-2             [-1, 40, 1024]               0\n",
      "            Linear-3              [-1, 40, 128]         131,200\n",
      "================================================================\n",
      "Total params: 263,296\n",
      "Trainable params: 263,296\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.66\n",
      "Params size (MB): 1.00\n",
      "Estimated Total Size (MB): 1.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# next we will implement the feed forward layer \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, n_hidden, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param n_hidden number of feed forward features in the hidden layer\"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # First linear layer\n",
    "        self.linear1 = nn.Linear(d_model, n_hidden)\n",
    "        \n",
    "        #Second Linear layer\n",
    "        self.linear2 = nn.Linear(n_hidden, d_model)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply first linear\n",
    "        x = self.linear1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "ff = FeedForward(128, 1024)\n",
    "summary(ff, (40, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         LayerNorm-1              [-1, 40, 128]             256\n",
      "================================================================\n",
      "Total params: 256\n",
      "Trainable params: 256\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(normalized_shape=input_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "    \n",
    "n = LayerNorm((128))\n",
    "summary(n, (40,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 40, 128]          16,384\n",
      "    MultiHeadSplit-2            [-1, 8, 40, 16]               0\n",
      "            Linear-3              [-1, 40, 128]          16,384\n",
      "    MultiHeadSplit-4            [-1, 8, 40, 16]               0\n",
      "            Linear-5              [-1, 40, 128]          16,384\n",
      "    MultiHeadSplit-6            [-1, 8, 40, 16]               0\n",
      "           Softmax-7            [-1, 8, 40, 40]               0\n",
      "     SelfAttention-8              [-1, 40, 128]               0\n",
      "         LayerNorm-9              [-1, 40, 128]             256\n",
      "        LayerNorm-10              [-1, 40, 128]               0\n",
      "          Dropout-11              [-1, 40, 128]               0\n",
      "           Linear-12             [-1, 40, 1024]         132,096\n",
      "          Dropout-13             [-1, 40, 1024]               0\n",
      "           Linear-14              [-1, 40, 128]         131,200\n",
      "      FeedForward-15              [-1, 40, 128]               0\n",
      "        LayerNorm-16              [-1, 40, 128]             256\n",
      "        LayerNorm-17              [-1, 40, 128]               0\n",
      "          Dropout-18              [-1, 40, 128]               0\n",
      "================================================================\n",
      "Total params: 312,960\n",
      "Trainable params: 312,960\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 1.31\n",
      "Params size (MB): 1.19\n",
      "Estimated Total Size (MB): 2.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# now we implement the encoder block \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This layer consists of self attention layer, followed by add and norm layer, then a feed forward layer, followed by add and norm layer \"\"\"\n",
    "    def __init__(self, d_model, d_k, heads, n_hidden=1024, dropout=0.1, bias=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "        self.bias = bias\n",
    "        \n",
    "        # define a self attention layer\n",
    "        self.self_attention = SelfAttention(d_model, heads, d_k, bias, dropout)\n",
    "        \n",
    "        # norm layer\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        \n",
    "        # feed forward layer\n",
    "        self.ff = FeedForward(d_model, n_hidden=n_hidden,dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x has a shape of (B, seq_length, d_model)\"\"\"\n",
    "        \n",
    "        att = self.self_attention(x)\n",
    "        \n",
    "        x = self.norm1(x + att)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        ff = self.ff(x)\n",
    "        \n",
    "        x = self.norm2(x + ff)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "enc = EncoderBlock(128, 16, 8)\n",
    "\n",
    "summary(enc, (40, 128))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 40, 128]          16,384\n",
      "    MultiHeadSplit-2            [-1, 8, 40, 16]               0\n",
      "            Linear-3              [-1, 40, 128]          16,384\n",
      "    MultiHeadSplit-4            [-1, 8, 40, 16]               0\n",
      "            Linear-5              [-1, 40, 128]          16,384\n",
      "    MultiHeadSplit-6            [-1, 8, 40, 16]               0\n",
      "           Softmax-7            [-1, 8, 40, 40]               0\n",
      "     SelfAttention-8              [-1, 40, 128]               0\n",
      "         LayerNorm-9              [-1, 40, 128]             256\n",
      "        LayerNorm-10              [-1, 40, 128]               0\n",
      "          Dropout-11              [-1, 40, 128]               0\n",
      "           Linear-12             [-1, 40, 1024]         132,096\n",
      "          Dropout-13             [-1, 40, 1024]               0\n",
      "           Linear-14              [-1, 40, 128]         131,200\n",
      "      FeedForward-15              [-1, 40, 128]               0\n",
      "        LayerNorm-16              [-1, 40, 128]             256\n",
      "        LayerNorm-17              [-1, 40, 128]               0\n",
      "          Dropout-18              [-1, 40, 128]               0\n",
      "     EncoderBlock-19              [-1, 40, 128]               0\n",
      "================================================================\n",
      "Total params: 312,960\n",
      "Trainable params: 312,960\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 1.35\n",
      "Params size (MB): 1.19\n",
      "Estimated Total Size (MB): 2.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the encoder sonsists of a positional embedding block + sequence of n encoder blocks \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, max_length, d_model, d_k, heads,n_encoders=1, n_hidden=1024, dropout=0.1, bias=False ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.pe = PositionalEncoding(d_model, max_length)\n",
    "      \n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            EncoderBlock(d_model, d_k, heads, n_hidden, dropout, bias) for i in range(n_encoders)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        \"\"\"x is of shape (B, seq_length, d_model)\n",
    "        :return output of shape (B, seq_length, d_model)\"\"\"\n",
    "\n",
    "        seq_length = x[1]\n",
    "        #pe = self.pe()[:seq_length,:]\n",
    "        # add positional information \n",
    "        x = x #+ pe\n",
    "        \n",
    "        # inject new input to encoder blocks \n",
    "        \n",
    "        for layer in self.enc_blocks:\n",
    "            x = layer(x)        \n",
    "        return x\n",
    "        \n",
    "enc = Encoder(40, 128, 16,8)\n",
    "summary(enc, (40,128))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1              [-1, 128, 40]               0\n",
      "         AvgPool1d-2               [-1, 128, 1]               0\n",
      "            Linear-3                    [-1, 5]             645\n",
      "           Softmax-4                    [-1, 5]               0\n",
      "================================================================\n",
      "Total params: 645\n",
      "Trainable params: 645\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# the full architechture consists of : input embedding + encoder + global average \n",
    "# we will experiment with global averaging and cls token [like BERT]\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, n_classes, d_model, seq_length):\n",
    "        super().__init__()\n",
    "        self.average_pooling = nn.AvgPool1d(kernel_size=seq_length, stride=seq_length)\n",
    "        self.flat = nn.Flatten(start_dim = 1, end_dim=2)\n",
    "        self.linear = nn.Linear(d_model, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        # add channel dimension \n",
    "        x = x.unsqueeze(dim=1)\n",
    "      \n",
    "        # transpose input \n",
    "        x = x.transpose(-1,-2)\n",
    "        \n",
    "        # flatten input \n",
    "        x = self.flat(x)\n",
    "    \n",
    "        # apply avgPool\n",
    "        x = self.average_pooling(x)\n",
    "        \n",
    "        # remove added dimension\n",
    "        x = x.squeeze(dim=-1)\n",
    "        \n",
    "        # apply linear layer \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # apply softmax \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "cls = Classifier(5, 128, 40)\n",
    "summary(cls, (40, 128))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 414)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the full arch consists of : input embedding + encoder + global average \n",
    "# we will experiment with global averaging and cls token [like BERT]\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier2(nn.Module):\n",
    "    def __init__(self, n_classes, d_model, seq_length):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply linear layer \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # apply softmax \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "cls2 = Classifier2(10, 128, 40)\n",
    "my_set[0][0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will encapsulate the whole work under one class \n",
    "\n",
    "class SERT(nn.Module):\n",
    "    def __init__(self,d_input,\n",
    "                 max_length,\n",
    "                 d_model,\n",
    "                 d_k,\n",
    "                 heads,\n",
    "                 n_classes,\n",
    "                 n_encoders=1,\n",
    "                 n_hidden=1024,\n",
    "                 dropout=0.1,\n",
    "                 bias=False,\n",
    "                 stride=10,\n",
    "                 kernel_size=16 ):\n",
    "        super().__init__()\n",
    "        self.input = input_embedding(d_input, d_model, stride, kernel_size)\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            Encoder(max_length,\n",
    "                    d_model,\n",
    "                    d_k,\n",
    "                    heads,\n",
    "                    n_encoders=1,\n",
    "                    n_hidden=1024,\n",
    "                    dropout=0.1,\n",
    "                    bias=False) for _ in range(n_encoders)\n",
    "        ])\n",
    "        self.output= Classifier(n_classes, d_model, max_length)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        return x\n",
    "        \n",
    "sert_model = SERT(d_input=64, max_length=35, d_model=128,d_k=16,heads=8,n_classes=9)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up the dataloader, the optimizer, and the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is split as follows:\n",
      "\n",
      "train : 4566(0.64)\n",
      "\n",
      "test : 1427(0.36)\n",
      "\n",
      "validation : 1142(0.16)\n"
     ]
    }
   ],
   "source": [
    "# first we will split the dataset into train_test_validation sets \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as torchdata \n",
    "\n",
    "\n",
    "def train_test_val_split(dataset, train_test,train_val):\n",
    "    \"\"\"splits dataset into 3 groups:\n",
    "    1- train dataset (size = train_test*train_val)\n",
    "    2 - test dataset (size = (1-train_test))\n",
    "    3- validation dataset (size = (train_test * (1-train_val))\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds, test_ds = torchdata.random_split(dataset, [int(train_test * len(dataset)), len(dataset) - int(train_test * len(dataset))])\n",
    "    train_ds, val_ds = torchdata.random_split(train_ds, [int(train_val * len(train_ds)), len(train_ds) - int(train_val * len(train_ds))])\n",
    "    train_size = train_test * train_val\n",
    "    test_size = 1 - train_size\n",
    "    val_size = train_test * (1 - train_val)\n",
    "    print(\"dataset is split as follows:\\n\\ntrain : {}({})\\n\\ntest : {}({})\\n\\nvalidation : {}({})\".format(len(train_ds), round(train_size,2), len(test_ds), round(test_size, 2), len(val_ds), round(val_size, 2)))\n",
    "    return train_ds, test_ds, val_ds\n",
    "\n",
    "\n",
    "train,test,val = train_test_val_split(my_set, 0.8, 0.8)\n",
    "\n",
    "# after splitting, we will define the dataloader\n",
    "train_dataloader = torchdata.DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
    "validation_dataloader = torchdata.DataLoader(dataset=val, batch_size=64, shuffle=True)\n",
    "test_dataloader = torchdata.DataLoader(dataset=test, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is split as follows:\n",
      "\n",
      "train : 4566(0.64)\n",
      "\n",
      "test : 1427(0.36)\n",
      "\n",
      "validation : 1142(0.16)\n",
      "cpu\n",
      "tensor([3, 3, 3, 0, 3, 3, 5, 3, 0, 0, 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 0, 0, 5,\n",
      "        3, 3, 3, 8, 5, 3, 0, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we will split the dataset into train_test_validation sets \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as torchdata \n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train_test_val_split(dataset, train_test,train_val):\n",
    "    \"\"\"splits dataset into 3 groups:\n",
    "    1- train dataset (size = train_test*train_val)\n",
    "    2 - test dataset (size = (1-train_test))\n",
    "    3- validation dataset (size = (train_test * (1-train_val))\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds, test_ds = torchdata.random_split(dataset, [int(train_test * len(dataset)), len(dataset) - int(train_test * len(dataset))])\n",
    "    train_ds, val_ds = torchdata.random_split(train_ds, [int(train_val * len(train_ds)), len(train_ds) - int(train_val * len(train_ds))])\n",
    "    train_size = train_test * train_val\n",
    "    test_size = 1 - train_size\n",
    "    val_size = train_test * (1 - train_val)\n",
    "    print(\"dataset is split as follows:\\n\\ntrain : {}({})\\n\\ntest : {}({})\\n\\nvalidation : {}({})\".format(len(train_ds), round(train_size,2), len(test_ds), round(test_size, 2), len(val_ds), round(val_size, 2)))\n",
    "    return train_ds, test_ds, val_ds\n",
    "\n",
    "\n",
    "train,test,val = train_test_val_split(my_set, 0.8, 0.8)\n",
    "\n",
    "# after splitting, we will define the dataloader\n",
    "train_dataloader = torchdata.DataLoader(dataset=train, batch_size=32, shuffle=True)\n",
    "validation_dataloader = torchdata.DataLoader(dataset=val, batch_size=32, shuffle=True)\n",
    "test_dataloader = torchdata.DataLoader(dataset=test, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# specify the optimizer (adam) and the loss function\n",
    "optimizer = torch.optim.Adam(sert_model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "sert_model = sert_model.to(device)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "y = batch[1].to(device)\n",
    "batch = batch[0]\n",
    "batch.shape\n",
    "y_pred = sert_model(batch.to(device))\n",
    "y_pred\n",
    "predicted = torch.argmax(y_pred, dim=1)\n",
    "print(predicted)\n",
    "(y == predicted).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0\n",
      "\n",
      "Epoch[1/10] batch[14/143] Loss[1.9360]\n",
      "Epoch[1/10] batch[29/143] Loss[1.9490]\n",
      "Epoch[1/10] batch[44/143] Loss[1.9633]\n",
      "Epoch[1/10] batch[59/143] Loss[1.9694]\n",
      "Epoch[1/10] batch[74/143] Loss[1.9395]\n",
      "Epoch[1/10] batch[89/143] Loss[1.9461]\n",
      "Epoch[1/10] batch[104/143] Loss[1.9295]\n",
      "Epoch[1/10] batch[119/143] Loss[1.9907]\n",
      "Epoch[1/10] batch[134/143] Loss[1.9882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▍                                       | 1/10 [00:23<03:28, 23.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1\n",
      "\n",
      "Epoch[2/10] batch[14/143] Loss[1.9543]\n",
      "Epoch[2/10] batch[29/143] Loss[1.9593]\n",
      "Epoch[2/10] batch[44/143] Loss[1.9410]\n",
      "Epoch[2/10] batch[59/143] Loss[1.9616]\n",
      "Epoch[2/10] batch[74/143] Loss[1.9576]\n",
      "Epoch[2/10] batch[89/143] Loss[1.9305]\n",
      "Epoch[2/10] batch[104/143] Loss[1.9487]\n",
      "Epoch[2/10] batch[119/143] Loss[1.9288]\n",
      "Epoch[2/10] batch[134/143] Loss[1.9336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 2/10 [00:46<03:07, 23.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2\n",
      "\n",
      "Epoch[3/10] batch[14/143] Loss[1.9261]\n",
      "Epoch[3/10] batch[29/143] Loss[1.9276]\n",
      "Epoch[3/10] batch[44/143] Loss[1.9266]\n",
      "Epoch[3/10] batch[59/143] Loss[1.9258]\n",
      "Epoch[3/10] batch[74/143] Loss[1.9466]\n",
      "Epoch[3/10] batch[89/143] Loss[1.9322]\n",
      "Epoch[3/10] batch[104/143] Loss[1.9698]\n",
      "Epoch[3/10] batch[119/143] Loss[1.9721]\n",
      "Epoch[3/10] batch[134/143] Loss[1.9556]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [01:09<02:42, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 3\n",
      "\n",
      "Epoch[4/10] batch[14/143] Loss[1.8929]\n",
      "Epoch[4/10] batch[29/143] Loss[1.9578]\n",
      "Epoch[4/10] batch[44/143] Loss[1.9222]\n",
      "Epoch[4/10] batch[59/143] Loss[1.9343]\n",
      "Epoch[4/10] batch[74/143] Loss[1.9127]\n",
      "Epoch[4/10] batch[89/143] Loss[1.9573]\n",
      "Epoch[4/10] batch[104/143] Loss[1.9330]\n",
      "Epoch[4/10] batch[119/143] Loss[1.9451]\n",
      "Epoch[4/10] batch[134/143] Loss[1.9461]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 4/10 [01:32<02:18, 23.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4\n",
      "\n",
      "Epoch[5/10] batch[14/143] Loss[1.9153]\n",
      "Epoch[5/10] batch[29/143] Loss[1.9250]\n",
      "Epoch[5/10] batch[44/143] Loss[1.9040]\n",
      "Epoch[5/10] batch[59/143] Loss[1.9096]\n",
      "Epoch[5/10] batch[74/143] Loss[1.9059]\n",
      "Epoch[5/10] batch[89/143] Loss[1.9399]\n",
      "Epoch[5/10] batch[104/143] Loss[1.9567]\n",
      "Epoch[5/10] batch[119/143] Loss[1.9518]\n",
      "Epoch[5/10] batch[134/143] Loss[1.9607]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 5/10 [01:55<01:55, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████                      | 5/10 [01:56<01:56, 23.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# loop through batches\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (X,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     21\u001b[0m     \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# move input to device\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device);\n\u001b[1;32m     24\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device);\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataset.py:362\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m, in \u001b[0;36mSER_dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     43\u001b[0m     \n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# get waveform from current index\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     waveform, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     resampler \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39msr, new_freq\u001b[38;5;241m=\u001b[39m SAMPLE_RATE)\n\u001b[1;32m     47\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m resampler(waveform)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torchaudio/_backend/utils.py:203\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torchaudio/_backend/soundfile.py:26\u001b[0m, in \u001b[0;36mSoundfileBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     18\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m     25\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torchaudio/_backend/soundfile_backend.py:230\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    227\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m _SUBTYPE2DTYPE[file_\u001b[38;5;241m.\u001b[39msubtype]\n\u001b[1;32m    229\u001b[0m     frames \u001b[38;5;241m=\u001b[39m file_\u001b[38;5;241m.\u001b[39m_prepare_read(frame_offset, \u001b[38;5;28;01mNone\u001b[39;00m, num_frames)\n\u001b[0;32m--> 230\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[43mfile_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     sample_rate \u001b[38;5;241m=\u001b[39m file_\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[1;32m    233\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(waveform)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[1;32m    894\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[0;32m--> 895\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39msizeof(ctype)\n\u001b[1;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1352\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[0;32m-> 1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1354\u001b[0m _error_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorcode)\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import  tqdm\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"runs/log_sert_model\")\n",
    "# set number of epochs (keep it small for initial investigation)\n",
    "num_epochs = 10\n",
    "\n",
    "torch.manual_seed(24)\n",
    "\n",
    "# add a timer \n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f\"epoch : {epoch}\\n\")\n",
    "    \n",
    "    acc_loss = 0.0\n",
    "    num_correct = 0\n",
    "    # loop through batches\n",
    "    for i, (X,y) in enumerate(train_dataloader):\n",
    "        \n",
    "        # move input to device\n",
    "        X = X.to(device);\n",
    "        y = y.to(device);\n",
    "        # set the model to train mode\n",
    "        sert_model.train();\n",
    "        \n",
    "        # get the predictions (forward pass)\n",
    "        y_pred = sert_model(X);\n",
    "        \n",
    "        # apply loss\n",
    "        current_loss = F.cross_entropy(y_pred, y);\n",
    "        \n",
    "        \n",
    "        \n",
    "        predicted = torch.argmax(y_pred, dim=1)\n",
    "        \n",
    "        # accumulate loss\n",
    "        acc_loss += current_loss;\n",
    "        \n",
    "        # get number of correct predictions of each batch\n",
    "        num_correct += (y == predicted).sum().item()\n",
    "        \n",
    "        # zero_grad the optimizer \n",
    "        optimizer.zero_grad();\n",
    "        \n",
    "        # step the loss\n",
    "        current_loss.backward();\n",
    "        \n",
    "        # step the optimizer\n",
    "        optimizer.step();\n",
    "        \n",
    "        if (i+1) % 15 == 0:\n",
    "            print(f\"Epoch[{epoch + 1}/{num_epochs}] batch[{i}/{len(train_dataloader)}] Loss[{acc_loss/15:.4f}]\")\n",
    "            writer.add_scalar('training loss', acc_loss / 15, epoch * len(train_dataloader) + i)\n",
    "            writer.add_scalar('training accuracy', num_correct / 15, epoch * len(train_dataloader) + i)\n",
    "            acc_loss = 0\n",
    "            num_correct = 0\n",
    "            \n",
    "            \n",
    "            # average loss per epoch\n",
    "acc_loss /= len(train_dataloader)\n",
    "\n",
    "### testing \n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "sert_model.eval()\n",
    "with torch.inference_mode():\n",
    "    for i, (X,y) in enumerate(validation_dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # get the prediction \n",
    "        y_pred = sert_model(X)\n",
    "\n",
    "        # calculate the loss\n",
    "        test_loss += loss(y_pred, y)\n",
    "\n",
    "    # show resutls \n",
    "    test_loss /= len(validation_dataloader)\n",
    "print(f\" train_losss : {acc_loss:4f} | test_loss: {test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "\n",
    "writer = SummaryWriter(\"runs/SER\")\n",
    "train_it = iter(train_dataloader)\n",
    "\n",
    "batch,_ = next(train_it)\n",
    "writer.add_graph(sert_model, batch.to(device))\n",
    "writer.close()\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get model hyperparameters into a dict\n",
    "2. train the model, storing all loss values and accuracies along the way in a list\n",
    "3. save the best performing model in an external dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def model_train(model,train_dataset, val_dataset, num_epochs, device, loss_fn, metric, optimizer, ext_dir):\n",
    "    \"\"\"returns information about the model, the results of training\n",
    "    :param ext_dir: directory to save the state of the best performing model according to the metric function\n",
    "    \"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'model name': model.__class__.__name__,\n",
    "        'device': device,\n",
    "        'loss_fn': loss_fn,\n",
    "        'metric': metric, \n",
    "        'optimizer':optimizer,\n",
    "        'num_epochs' : num_epochs,\n",
    "    }\n",
    "    \n",
    "    batch_size = train_dataset.batch_size\n",
    "    \n",
    "    print(f\"start training with :\\n total epochs: {num_epochs}\\n batch size: {train_dataset.batch_size}\\n total batches: {len(train_dataset)}\")\n",
    "    \n",
    "    #set a timer to measure the training time \n",
    "    start_timer = time.time()\n",
    "    \n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # log stride = 10% of the number of batches to loop through, this is used to print results while training \n",
    "    log_stride = int(0.1 * len(train_dataset))\n",
    "    \n",
    "    # add model graph to tensorboard\n",
    "    batch, _ = next(iter(train_dataset))\n",
    "    writer = SummaryWriter(f\"runs/log_sert_model\")\n",
    "    writer.add_graph(model, batch.to(device))\n",
    "    \n",
    "    \n",
    "    # save the best performing model\n",
    "    max_acc = 0.0\n",
    "    \n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # we will calculate average loss per epoch, average accuracy per epoch\n",
    "        loss_epoch = 0.0\n",
    "        accuracy_epoch = 0.0\n",
    "        \n",
    "        # average metrics per log_stride\n",
    "        loss_stride = 0.0\n",
    "        correct_stride = 0\n",
    "        \n",
    "        # save metrics for logging\n",
    "        train_loss =[]\n",
    "        train_acc = []\n",
    "        eval_loss = []\n",
    "        eval_acc = []\n",
    "            \n",
    "        model.train()\n",
    "        for it, (X, y) in enumerate(train_dataset):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # calculate the loss\n",
    "            loss =loss_fn(y_pred, y)\n",
    "            loss_stride += loss\n",
    "            train_loss.append((loss, epoch, it))\n",
    "            \n",
    "            \n",
    "            # get correct predictions\n",
    "            predictions = torch.argmax(y_pred, dim=1)\n",
    "           \n",
    "            \n",
    "            correct_stride += (y == predictions).sum().item()\n",
    "            train_acc.append(((correct_stride)/batch_size, epoch, it))\n",
    "            \n",
    "            # zero grad the optim.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if (it + 1) % log_stride == 0:\n",
    "                # print results\n",
    "                print(f\"epoch[{epoch + 1}/{num_epochs}] | batch[{it + 1}/{len(train_dataset)}] train_loss: {loss_stride/ (log_stride):.4f} | accuracy: {correct_stride/(log_stride * batch_size)}\")\n",
    "                \n",
    "                # send scalars to tensorboard\n",
    "                writer.add_scalar('train_loss',loss_stride/ (log_stride), epoch * len(train_dataset) + it)\n",
    "                writer.add_scalar('train accuracy', correct_stride/(log_stride * batch_size), epoch * len(train_dataset) + it)\n",
    "                \n",
    "                # reset parameters\n",
    "                loss_epoch += loss_stride\n",
    "                accuracy_epoch += correct_stride\n",
    "                loss_stride = 0.0\n",
    "                correct_stride = 0\n",
    "        \n",
    "        print(\"validation:\")\n",
    "        model.eval()\n",
    "        loss_val = 0.0\n",
    "        acc_val = 0.0\n",
    "        \n",
    "        \n",
    "        with torch.inference_mode() :\n",
    "            correct = 0\n",
    "            for (X_test, y_test) in val_dataset:\n",
    "                X_test = X_test.to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                \n",
    "                y_pred = model(X_test)\n",
    "                loss_val += loss_fn(y_pred, y_test)\n",
    "                predictions = torch.argmax(y_pred, dim=1)\n",
    "                \n",
    "                correct += (predictions == y_test).sum().item()\n",
    "                \n",
    "        eval_loss.append((loss_val / len(val_dataset), epoch))\n",
    "        eval_acc.append(((correct/(len(val_dataset) * batch_size)), epoch))\n",
    "            \n",
    "        train_accuracy = accuracy_epoch/(batch_size * len(train_dataset))\n",
    "        print(f\"epoch: {epoch} summary:\\n train_loss: {loss_epoch / len(train_dataset)}, validation loss: {loss_val / len(val_dataset)}, train accuracy: {accuracy_epoch/(len(train_dataset) * batch_size)}\")\n",
    "        \n",
    "        summary['train_loss'] = loss_epoch / len(train_dataset)\n",
    "        summary['validation_loss'] = loss_val / len(val_dataset)\n",
    "        summary['validation_accuracy'] = correct/(len(val_dataset) * batch_size)\n",
    "        writer.add_scalar('validation_loss',eval_loss[-1][0],  epoch)\n",
    "        writer.add_scalar('validation_accuracy',eval_acc[-1][0],  epoch)\n",
    "        \n",
    "        if train_accuracy > max_acc:\n",
    "            max_acc = train_accuracy\n",
    "            torch.save(model.state_dict(), ext_dir)\n",
    "    \n",
    "    total_time = (time.time() - start_timer)\n",
    "    summary['total_time'] = total_time\n",
    "    writer.close()\n",
    "    return summary, train_accuracy, train_loss, eval_acc, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is split as follows:\n",
      "\n",
      "train : 4566(0.64)\n",
      "\n",
      "test : 1427(0.36)\n",
      "\n",
      "validation : 1142(0.16)\n",
      "start training with :\n",
      " total epochs: 10\n",
      " batch size: 64\n",
      " total batches: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/10] | batch[7/72] train_loss: 2.1969 | accuracy: 0.11160714285714286\n",
      "epoch[1/10] | batch[14/72] train_loss: 2.1926 | accuracy: 0.15625\n",
      "epoch[1/10] | batch[21/72] train_loss: 2.1837 | accuracy: 0.20758928571428573\n",
      "epoch[1/10] | batch[28/72] train_loss: 2.1761 | accuracy: 0.19419642857142858\n",
      "epoch[1/10] | batch[35/72] train_loss: 2.1679 | accuracy: 0.22321428571428573\n",
      "epoch[1/10] | batch[42/72] train_loss: 2.1595 | accuracy: 0.21205357142857142\n",
      "epoch[1/10] | batch[49/72] train_loss: 2.1546 | accuracy: 0.203125\n",
      "epoch[1/10] | batch[56/72] train_loss: 2.1500 | accuracy: 0.21875\n",
      "epoch[1/10] | batch[63/72] train_loss: 2.1428 | accuracy: 0.25669642857142855\n",
      "epoch[1/10] | batch[70/72] train_loss: 2.1335 | accuracy: 0.25\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▍                                       | 1/10 [00:21<03:15, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 summary:\n",
      " train_loss: 2.105602741241455, validation loss: 2.13472056388855, train accuracy: 0.1976996527777778\n",
      "epoch[2/10] | batch[7/72] train_loss: 2.1178 | accuracy: 0.26339285714285715\n",
      "epoch[2/10] | batch[14/72] train_loss: 2.1168 | accuracy: 0.27232142857142855\n",
      "epoch[2/10] | batch[21/72] train_loss: 2.1202 | accuracy: 0.23660714285714285\n",
      "epoch[2/10] | batch[28/72] train_loss: 2.1119 | accuracy: 0.24776785714285715\n",
      "epoch[2/10] | batch[35/72] train_loss: 2.1109 | accuracy: 0.2544642857142857\n",
      "epoch[2/10] | batch[42/72] train_loss: 2.1106 | accuracy: 0.25\n",
      "epoch[2/10] | batch[49/72] train_loss: 2.1127 | accuracy: 0.22098214285714285\n",
      "epoch[2/10] | batch[56/72] train_loss: 2.1150 | accuracy: 0.22991071428571427\n",
      "epoch[2/10] | batch[63/72] train_loss: 2.1045 | accuracy: 0.2611607142857143\n",
      "epoch[2/10] | batch[70/72] train_loss: 2.0972 | accuracy: 0.25223214285714285\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 2/10 [00:47<03:13, 24.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 summary:\n",
      " train_loss: 2.053112030029297, validation loss: 2.103925943374634, train accuracy: 0.2419704861111111\n",
      "epoch[3/10] | batch[7/72] train_loss: 2.0841 | accuracy: 0.28348214285714285\n",
      "epoch[3/10] | batch[14/72] train_loss: 2.0839 | accuracy: 0.2767857142857143\n",
      "epoch[3/10] | batch[21/72] train_loss: 2.0915 | accuracy: 0.25\n",
      "epoch[3/10] | batch[28/72] train_loss: 2.0826 | accuracy: 0.29017857142857145\n",
      "epoch[3/10] | batch[35/72] train_loss: 2.0871 | accuracy: 0.296875\n",
      "epoch[3/10] | batch[42/72] train_loss: 2.0888 | accuracy: 0.265625\n",
      "epoch[3/10] | batch[49/72] train_loss: 2.0934 | accuracy: 0.25223214285714285\n",
      "epoch[3/10] | batch[56/72] train_loss: 2.0993 | accuracy: 0.25\n",
      "epoch[3/10] | batch[63/72] train_loss: 2.0865 | accuracy: 0.3013392857142857\n",
      "epoch[3/10] | batch[70/72] train_loss: 2.0783 | accuracy: 0.29910714285714285\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [01:16<03:03, 26.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 summary:\n",
      " train_loss: 2.0295567512512207, validation loss: 2.0850465297698975, train accuracy: 0.2688802083333333\n",
      "epoch[4/10] | batch[7/72] train_loss: 2.0670 | accuracy: 0.3125\n",
      "epoch[4/10] | batch[14/72] train_loss: 2.0639 | accuracy: 0.3325892857142857\n",
      "epoch[4/10] | batch[21/72] train_loss: 2.0715 | accuracy: 0.31026785714285715\n",
      "epoch[4/10] | batch[28/72] train_loss: 2.0635 | accuracy: 0.32589285714285715\n",
      "epoch[4/10] | batch[35/72] train_loss: 2.0710 | accuracy: 0.3325892857142857\n",
      "epoch[4/10] | batch[42/72] train_loss: 2.0711 | accuracy: 0.31473214285714285\n",
      "epoch[4/10] | batch[49/72] train_loss: 2.0761 | accuracy: 0.29017857142857145\n",
      "epoch[4/10] | batch[56/72] train_loss: 2.0848 | accuracy: 0.25892857142857145\n",
      "epoch[4/10] | batch[63/72] train_loss: 2.0668 | accuracy: 0.3325892857142857\n",
      "epoch[4/10] | batch[70/72] train_loss: 2.0639 | accuracy: 0.33482142857142855\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 4/10 [01:43<02:40, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 summary:\n",
      " train_loss: 2.0124616622924805, validation loss: 2.068875312805176, train accuracy: 0.3057725694444444\n",
      "epoch[5/10] | batch[7/72] train_loss: 2.0519 | accuracy: 0.34375\n",
      "epoch[5/10] | batch[14/72] train_loss: 2.0476 | accuracy: 0.34598214285714285\n",
      "epoch[5/10] | batch[21/72] train_loss: 2.0536 | accuracy: 0.36160714285714285\n",
      "epoch[5/10] | batch[28/72] train_loss: 2.0455 | accuracy: 0.36160714285714285\n",
      "epoch[5/10] | batch[35/72] train_loss: 2.0561 | accuracy: 0.34151785714285715\n",
      "epoch[5/10] | batch[42/72] train_loss: 2.0549 | accuracy: 0.328125\n",
      "epoch[5/10] | batch[49/72] train_loss: 2.0603 | accuracy: 0.31026785714285715\n",
      "epoch[5/10] | batch[56/72] train_loss: 2.0704 | accuracy: 0.27901785714285715\n",
      "epoch[5/10] | batch[63/72] train_loss: 2.0448 | accuracy: 0.3638392857142857\n",
      "epoch[5/10] | batch[70/72] train_loss: 2.0523 | accuracy: 0.32142857142857145\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████                      | 5/10 [02:12<02:17, 27.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 summary:\n",
      " train_loss: 1.9966875314712524, validation loss: 2.0543887615203857, train accuracy: 0.3263888888888889\n",
      "epoch[6/10] | batch[7/72] train_loss: 2.0350 | accuracy: 0.3705357142857143\n",
      "epoch[6/10] | batch[14/72] train_loss: 2.0333 | accuracy: 0.36830357142857145\n",
      "epoch[6/10] | batch[21/72] train_loss: 2.0397 | accuracy: 0.38392857142857145\n",
      "epoch[6/10] | batch[28/72] train_loss: 2.0299 | accuracy: 0.3794642857142857\n",
      "epoch[6/10] | batch[35/72] train_loss: 2.0446 | accuracy: 0.3482142857142857\n",
      "epoch[6/10] | batch[42/72] train_loss: 2.0413 | accuracy: 0.35267857142857145\n",
      "epoch[6/10] | batch[49/72] train_loss: 2.0497 | accuracy: 0.31026785714285715\n",
      "epoch[6/10] | batch[56/72] train_loss: 2.0570 | accuracy: 0.29910714285714285\n",
      "epoch[6/10] | batch[63/72] train_loss: 2.0284 | accuracy: 0.3861607142857143\n",
      "epoch[6/10] | batch[70/72] train_loss: 2.0440 | accuracy: 0.33705357142857145\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 6/10 [02:42<01:52, 28.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 summary:\n",
      " train_loss: 1.9836055040359497, validation loss: 2.0445990562438965, train accuracy: 0.34375\n",
      "epoch[7/10] | batch[7/72] train_loss: 2.0244 | accuracy: 0.38392857142857145\n",
      "epoch[7/10] | batch[14/72] train_loss: 2.0229 | accuracy: 0.37723214285714285\n",
      "epoch[7/10] | batch[21/72] train_loss: 2.0310 | accuracy: 0.38392857142857145\n",
      "epoch[7/10] | batch[28/72] train_loss: 2.0198 | accuracy: 0.3950892857142857\n",
      "epoch[7/10] | batch[35/72] train_loss: 2.0374 | accuracy: 0.3549107142857143\n",
      "epoch[7/10] | batch[42/72] train_loss: 2.0328 | accuracy: 0.36160714285714285\n",
      "epoch[7/10] | batch[49/72] train_loss: 2.0423 | accuracy: 0.31473214285714285\n",
      "epoch[7/10] | batch[56/72] train_loss: 2.0481 | accuracy: 0.31026785714285715\n",
      "epoch[7/10] | batch[63/72] train_loss: 2.0197 | accuracy: 0.3861607142857143\n",
      "epoch[7/10] | batch[70/72] train_loss: 2.0355 | accuracy: 0.3392857142857143\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████▊             | 7/10 [03:11<01:25, 28.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 summary:\n",
      " train_loss: 1.9749565124511719, validation loss: 2.0365703105926514, train accuracy: 0.3506944444444444\n",
      "epoch[8/10] | batch[7/72] train_loss: 2.0161 | accuracy: 0.3794642857142857\n",
      "epoch[8/10] | batch[14/72] train_loss: 2.0141 | accuracy: 0.38392857142857145\n",
      "epoch[8/10] | batch[21/72] train_loss: 2.0227 | accuracy: 0.3794642857142857\n",
      "epoch[8/10] | batch[28/72] train_loss: 2.0109 | accuracy: 0.40625\n",
      "epoch[8/10] | batch[35/72] train_loss: 2.0327 | accuracy: 0.359375\n",
      "epoch[8/10] | batch[42/72] train_loss: 2.0259 | accuracy: 0.3705357142857143\n",
      "epoch[8/10] | batch[49/72] train_loss: 2.0363 | accuracy: 0.3236607142857143\n",
      "epoch[8/10] | batch[56/72] train_loss: 2.0421 | accuracy: 0.33482142857142855\n",
      "epoch[8/10] | batch[63/72] train_loss: 2.0128 | accuracy: 0.39285714285714285\n",
      "epoch[8/10] | batch[70/72] train_loss: 2.0291 | accuracy: 0.35044642857142855\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████▏        | 8/10 [03:40<00:57, 28.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 summary:\n",
      " train_loss: 1.9680355787277222, validation loss: 2.03070330619812, train accuracy: 0.3578559027777778\n",
      "epoch[9/10] | batch[7/72] train_loss: 2.0098 | accuracy: 0.3861607142857143\n",
      "epoch[9/10] | batch[14/72] train_loss: 2.0084 | accuracy: 0.38839285714285715\n",
      "epoch[9/10] | batch[21/72] train_loss: 2.0163 | accuracy: 0.37276785714285715\n",
      "epoch[9/10] | batch[28/72] train_loss: 2.0035 | accuracy: 0.4107142857142857\n",
      "epoch[9/10] | batch[35/72] train_loss: 2.0298 | accuracy: 0.359375\n",
      "epoch[9/10] | batch[42/72] train_loss: 2.0208 | accuracy: 0.36607142857142855\n",
      "epoch[9/10] | batch[49/72] train_loss: 2.0318 | accuracy: 0.33035714285714285\n",
      "epoch[9/10] | batch[56/72] train_loss: 2.0369 | accuracy: 0.33482142857142855\n",
      "epoch[9/10] | batch[63/72] train_loss: 2.0077 | accuracy: 0.38839285714285715\n",
      "epoch[9/10] | batch[70/72] train_loss: 2.0233 | accuracy: 0.3549107142857143\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|███████████████████████████████████████▌    | 9/10 [04:06<00:28, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 summary:\n",
      " train_loss: 1.9627538919448853, validation loss: 2.026277780532837, train accuracy: 0.3589409722222222\n",
      "epoch[10/10] | batch[7/72] train_loss: 2.0052 | accuracy: 0.3950892857142857\n",
      "epoch[10/10] | batch[14/72] train_loss: 2.0033 | accuracy: 0.39732142857142855\n",
      "epoch[10/10] | batch[21/72] train_loss: 2.0103 | accuracy: 0.375\n",
      "epoch[10/10] | batch[28/72] train_loss: 1.9992 | accuracy: 0.40625\n",
      "epoch[10/10] | batch[35/72] train_loss: 2.0266 | accuracy: 0.359375\n",
      "epoch[10/10] | batch[42/72] train_loss: 2.0171 | accuracy: 0.37723214285714285\n",
      "epoch[10/10] | batch[49/72] train_loss: 2.0268 | accuracy: 0.33035714285714285\n",
      "epoch[10/10] | batch[56/72] train_loss: 2.0325 | accuracy: 0.328125\n",
      "epoch[10/10] | batch[63/72] train_loss: 2.0037 | accuracy: 0.39285714285714285\n",
      "epoch[10/10] | batch[70/72] train_loss: 2.0188 | accuracy: 0.35714285714285715\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [04:33<00:00, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 summary:\n",
      " train_loss: 1.9583990573883057, validation loss: 2.0220940113067627, train accuracy: 0.3615451388888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = SERT(64,40, d_model=128,d_k= 16, heads=8,n_classes=9,n_encoders=1,dropout=0, n_hidden=768)\n",
    "train,test,val = train_test_val_split(my_set, 0.8, 0.8)\n",
    "\n",
    "# after splitting, we will define the dataloader\n",
    "train_dataloader = torchdata.DataLoader(dataset=train, batch_size=64)\n",
    "validation_dataloader = torchdata.DataLoader(dataset=val, batch_size=64, shuffle=True)\n",
    "test_dataloader = torchdata.DataLoader(dataset=test, batch_size=32, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001)\n",
    "summary = model_train(model, train_dataloader, validation_dataloader, num_epochs=10, device=device, loss_fn=loss_fn, metric=acc_loss, optimizer=optimizer, ext_dir='./model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training with :\n",
      " total epochs: 10\n",
      " batch size: 32\n",
      " total batches: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/10] | batch[14/143] train_loss: 1.9815 | accuracy: 0.390625\n",
      "epoch[1/10] | batch[28/143] train_loss: 1.9821 | accuracy: 0.38839285714285715\n",
      "epoch[1/10] | batch[42/143] train_loss: 1.9700 | accuracy: 0.41294642857142855\n",
      "epoch[1/10] | batch[56/143] train_loss: 1.9368 | accuracy: 0.4486607142857143\n",
      "epoch[1/10] | batch[70/143] train_loss: 1.9655 | accuracy: 0.4107142857142857\n",
      "epoch[1/10] | batch[84/143] train_loss: 1.9938 | accuracy: 0.37276785714285715\n",
      "epoch[1/10] | batch[98/143] train_loss: 1.9332 | accuracy: 0.4575892857142857\n",
      "epoch[1/10] | batch[112/143] train_loss: 1.9581 | accuracy: 0.421875\n",
      "epoch[1/10] | batch[126/143] train_loss: 1.9503 | accuracy: 0.4419642857142857\n",
      "epoch[1/10] | batch[140/143] train_loss: 1.9496 | accuracy: 0.41964285714285715\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▍                                       | 1/10 [00:23<03:27, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 summary:\n",
      " train_loss: 1.92090904712677, validation loss: 1.992280125617981, train accuracy: 0.40777972027972026\n",
      "epoch[2/10] | batch[14/143] train_loss: 1.9378 | accuracy: 0.4419642857142857\n",
      "epoch[2/10] | batch[28/143] train_loss: 1.9476 | accuracy: 0.43080357142857145\n",
      "epoch[2/10] | batch[42/143] train_loss: 1.9632 | accuracy: 0.4174107142857143\n",
      "epoch[2/10] | batch[56/143] train_loss: 1.9671 | accuracy: 0.40848214285714285\n",
      "epoch[2/10] | batch[70/143] train_loss: 1.9364 | accuracy: 0.45982142857142855\n",
      "epoch[2/10] | batch[84/143] train_loss: 1.9705 | accuracy: 0.4174107142857143\n",
      "epoch[2/10] | batch[98/143] train_loss: 1.9910 | accuracy: 0.375\n",
      "epoch[2/10] | batch[112/143] train_loss: 1.9540 | accuracy: 0.4107142857142857\n",
      "epoch[2/10] | batch[126/143] train_loss: 1.9753 | accuracy: 0.40401785714285715\n",
      "epoch[2/10] | batch[140/143] train_loss: 1.9698 | accuracy: 0.4174107142857143\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 2/10 [00:56<03:52, 29.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 summary:\n",
      " train_loss: 1.9201096296310425, validation loss: 1.990226149559021, train accuracy: 0.40952797202797203\n",
      "epoch[3/10] | batch[14/143] train_loss: 1.9954 | accuracy: 0.3705357142857143\n",
      "epoch[3/10] | batch[28/143] train_loss: 1.9799 | accuracy: 0.4017857142857143\n",
      "epoch[3/10] | batch[42/143] train_loss: 1.9594 | accuracy: 0.4375\n",
      "epoch[3/10] | batch[56/143] train_loss: 1.9687 | accuracy: 0.41294642857142855\n",
      "epoch[3/10] | batch[70/143] train_loss: 1.9409 | accuracy: 0.42857142857142855\n",
      "epoch[3/10] | batch[84/143] train_loss: 1.9395 | accuracy: 0.4375\n",
      "epoch[3/10] | batch[98/143] train_loss: 1.9481 | accuracy: 0.44642857142857145\n",
      "epoch[3/10] | batch[112/143] train_loss: 1.9766 | accuracy: 0.38839285714285715\n",
      "epoch[3/10] | batch[126/143] train_loss: 1.9349 | accuracy: 0.4486607142857143\n",
      "epoch[3/10] | batch[140/143] train_loss: 1.9738 | accuracy: 0.3861607142857143\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 3/10 [01:28<03:34, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 summary:\n",
      " train_loss: 1.920564889907837, validation loss: 1.9880449771881104, train accuracy: 0.4071241258741259\n",
      "epoch[4/10] | batch[14/143] train_loss: 1.9496 | accuracy: 0.43080357142857145\n",
      "epoch[4/10] | batch[28/143] train_loss: 1.9688 | accuracy: 0.39955357142857145\n",
      "epoch[4/10] | batch[42/143] train_loss: 1.9853 | accuracy: 0.390625\n",
      "epoch[4/10] | batch[56/143] train_loss: 1.9833 | accuracy: 0.3861607142857143\n",
      "epoch[4/10] | batch[70/143] train_loss: 1.9178 | accuracy: 0.46875\n",
      "epoch[4/10] | batch[84/143] train_loss: 1.9402 | accuracy: 0.45535714285714285\n",
      "epoch[4/10] | batch[98/143] train_loss: 1.9556 | accuracy: 0.41294642857142855\n",
      "epoch[4/10] | batch[112/143] train_loss: 1.9610 | accuracy: 0.41517857142857145\n",
      "epoch[4/10] | batch[126/143] train_loss: 1.9610 | accuracy: 0.41294642857142855\n",
      "epoch[4/10] | batch[140/143] train_loss: 1.9553 | accuracy: 0.43080357142857145\n",
      "validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████▏                              | 3/10 [01:57<04:34, 39.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macc_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[164], line 106\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(model, train_dataset, val_dataset, num_epochs, device, loss_fn, metric, optimizer, ext_dir)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode() :\n\u001b[1;32m    105\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (X_test, y_test) \u001b[38;5;129;01min\u001b[39;00m val_dataset:\n\u001b[1;32m    107\u001b[0m         X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    108\u001b[0m         y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataset.py:362\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mSER_dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     43\u001b[0m     \n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# get waveform from current index\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     waveform, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilenames[index])\n\u001b[0;32m---> 46\u001b[0m     resampler \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResample\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m resampler(waveform)\n\u001b[1;32m     48\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_cut_if_necessary(waveform)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torchaudio/transforms/_transforms.py:957\u001b[0m, in \u001b[0;36mResample.__init__\u001b[0;34m(self, orig_freq, new_freq, resampling_method, lowpass_filter_width, rolloff, beta, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m=\u001b[39m beta\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_freq \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_freq:\n\u001b[0;32m--> 957\u001b[0m     kernel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m \u001b[43m_get_sinc_resample_kernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlowpass_filter_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrolloff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresampling_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernel)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/torchaudio/functional/functional.py:1424\u001b[0m, in \u001b[0;36m_get_sinc_resample_kernel\u001b[0;34m(orig_freq, new_freq, gcd, lowpass_filter_width, rolloff, resampling_method, beta, device, dtype)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# we do not use built in torch windows here as we need to evaluate the window\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# at specific positions, not over a regular grid.\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resampling_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msinc_interp_hann\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1424\u001b[0m     window \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlowpass_filter_width\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# sinc_interp_kaiser\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summary1 = model_train(model, train_dataloader, validation_dataloader, num_epochs=10, device=device, loss_fn=loss_fn, metric=acc_loss, optimizer=optimizer, ext_dir='./model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. problem: fluctuating loss\n",
    "\n",
    "## possible causes:\n",
    "- dropout rate ( easy )\n",
    "- small dataset ( hard )\n",
    "- unnormalized data ( easy )\n",
    "- small batch size ( easy )\n",
    "\n",
    "\n",
    "\n",
    "# 2. problem: small accuracy:\n",
    "## possible causes:\n",
    "\n",
    "- needs fine-tuning hyperparameters ( takes time )\n",
    "- features are not suitable ( takes experiments )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = SERT(d_input= 64, max_length= 40, d_model= 128,d_k=16, heads=8, n_classes=9,dropout=0 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
